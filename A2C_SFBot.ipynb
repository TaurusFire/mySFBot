{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import retro\n",
    "import time\n",
    "import os\n",
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import optuna\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import tensorboard\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the environment: observation space, action space, preprocessing steps, what happens every timestep, and resetting the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFighter(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        #the observation space is a 84x84 box with each value correp to a colour\n",
    "        self.observation_space = Box(low=0, high=255, \n",
    "                                     shape=(84,84,1), dtype=np.uint8)\n",
    "        \n",
    "        #action space of 12-long vectors where each action corresps to a 0 or a 1\n",
    "        self.action_space = MultiBinary(12)\n",
    "        \n",
    "        #start up an instance of the game\n",
    "        #use restricted actions ensures that only valid button combinations are chosen\n",
    "        self.game = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\",\n",
    "                               use_restricted_actions = retro.Actions.FILTERED)\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        #turn to grey, resize, and regain the channels value\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), interpolation= cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels\n",
    "    \n",
    "    def step(self,action):\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "\n",
    "        obs = self.preprocess(obs)\n",
    "\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        #reward function is score delta\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def reset(self):\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Set score to dero \n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logs for tensorboard data and hyperparameter models\n",
    "LOG_DIR = './A2Clogs/'\n",
    "OPT_DIR = './A2Copt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveA2C(trial):\n",
    "    #generates an example set of hyperparamaters\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps',2048,8192),\n",
    "        'gamma': trial.suggest_float('gamma',0.8,0.9999, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate',1e-6,1e-5, log=True),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda',0.8,0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    #evaluate the agent's performance when it trains using different sets of hyperparameters\n",
    "    try:\n",
    "        model_params = objectiveA2C(trial)\n",
    "\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env =  DummyVecEnv([lambda:env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        \n",
    "        model = A2C('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        print(\"model_made\")\n",
    "        model.learn(total_timesteps=100000)\n",
    "        print(\"model learned\")\n",
    "\n",
    "        mean_reward, __ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "    \n",
    "    except Exception as e:\n",
    "       return -1000\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 13:30:15,628] A new study created in memory with name: no-name-8aad12ad-e796-440e-8973-8f6d4433ad0c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 13:35:12,965] Trial 0 finished with value: 4500.0 and parameters: {'n_steps': 7941, 'gamma': 0.8663412679242408, 'learning_rate': 1.3756970061853513e-06, 'gae_lambda': 0.9095683342511351}. Best is trial 0 with value: 4500.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 13:41:04,588] Trial 1 finished with value: 18100.0 and parameters: {'n_steps': 5834, 'gamma': 0.9532514222680888, 'learning_rate': 3.8509210607144295e-06, 'gae_lambda': 0.9790912340563886}. Best is trial 1 with value: 18100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 13:46:34,858] Trial 2 finished with value: 0.0 and parameters: {'n_steps': 4362, 'gamma': 0.98268551561208, 'learning_rate': 7.127695420229584e-06, 'gae_lambda': 0.8321319788402167}. Best is trial 1 with value: 18100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 13:51:30,202] Trial 3 finished with value: 1000.0 and parameters: {'n_steps': 4252, 'gamma': 0.9806151632514849, 'learning_rate': 4.656764298235243e-06, 'gae_lambda': 0.9583989449522734}. Best is trial 1 with value: 18100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 13:57:01,079] Trial 4 finished with value: 1900.0 and parameters: {'n_steps': 4149, 'gamma': 0.9531188114863924, 'learning_rate': 5.079551467056385e-06, 'gae_lambda': 0.9886142257354342}. Best is trial 1 with value: 18100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 14:02:25,797] Trial 5 finished with value: 1000.0 and parameters: {'n_steps': 5082, 'gamma': 0.9200829751203636, 'learning_rate': 6.068525608623374e-06, 'gae_lambda': 0.8620870837237014}. Best is trial 1 with value: 18100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 14:07:59,345] Trial 6 finished with value: 1800.0 and parameters: {'n_steps': 6588, 'gamma': 0.8530801768400269, 'learning_rate': 2.9679885414326437e-06, 'gae_lambda': 0.8528718224668003}. Best is trial 1 with value: 18100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 14:13:23,528] Trial 7 finished with value: 200.0 and parameters: {'n_steps': 7167, 'gamma': 0.9900433416017995, 'learning_rate': 8.287108694225938e-06, 'gae_lambda': 0.9137101821077287}. Best is trial 1 with value: 18100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 14:18:37,957] Trial 8 finished with value: 4600.0 and parameters: {'n_steps': 2054, 'gamma': 0.9502517312474695, 'learning_rate': 1.303061036066252e-06, 'gae_lambda': 0.9069686611891224}. Best is trial 1 with value: 18100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_made\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 14:23:48,818] Trial 9 finished with value: 4600.0 and parameters: {'n_steps': 5875, 'gamma': 0.9822658979737727, 'learning_rate': 1.5233019714238133e-06, 'gae_lambda': 0.9434599087519566}. Best is trial 1 with value: 18100.0.\n"
     ]
    }
   ],
   "source": [
    "#create the experiment/study. since returning a positive value, want to maximise the function. \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the best set of hyperparameters (trial 1) and set this to model_params\n",
    "\"\"\"study.best_params ={'n_steps': 5834,\n",
    "  'gamma': 0.9532514222680888,\n",
    "  'learning_rate': 3.8509210607144295e-06,\n",
    "  'gae_lambda': 0.9790912340563886}\"\"\"\n",
    "\n",
    "model_params = {'n_steps': 5834,\n",
    "  'gamma': 0.9532514222680888,\n",
    "  'learning_rate': 3.8509210607144295e-06,\n",
    "  'gae_lambda': 0.9790912340563886}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the n_steps parameter a multiple of 64\n",
    "5834/64 #=91.15625\n",
    "91*64 #=5824\n",
    "model_params['n_steps'] = 5824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where we save the intermediate saved models\n",
    "CHECKPOINT_DIR = \"./A2Ctrain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model every 100,000 steps at checkpoint_dir\n",
    "callback = TrainAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create env and process for stacking\n",
    "\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda:env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f22e63b9d30>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load and train the model\n",
    "model = A2C('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "model.load(os.path.join(OPT_DIR, 'trial_1_best_model.zip'))\n",
    "model.learn(total_timesteps=1000000, callback= callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f5442a65580>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load fully trained model\n",
    "model.load(\"./A2Ctrain/best_model_1000000.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain mean reward after 30 episodes\n",
    "mean_reward,_ = evaluate_policy(model, env, n_eval_episodes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4200.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Run the game, obtaining the next actions using the model.\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
