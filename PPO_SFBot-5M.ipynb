{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file is identical to PPO_SFBot except that the model is trained for 5M timesteps. \n",
    "Please refer to the comments of that file for an explanation of the code.\"\"\"\n",
    "\n",
    "#install dependencies\n",
    "import retro\n",
    "import time\n",
    "import os\n",
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.observation_space = Box(low=0, high=255, \n",
    "                                     shape=(84,84,1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "        self.game = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\",\n",
    "                               use_restricted_actions = retro.Actions.FILTERED)\n",
    "\n",
    "    def step(self,action):\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        self.score=0\n",
    "        return obs\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), interpolation= cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intend to use same best hyperparamaters as we did for 1M trained model\n",
    "#but training tensorboard logs will be in a new folder\n",
    "LOG_DIR = './PPOlogs3/'\n",
    "OPT_DIR = './PPOopt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    return {\n",
    "\n",
    "        'n_steps': trial.suggest_int('n_steps',2048,8192),\n",
    "        'gamma': trial.suggest_float('gamma',0.8,0.9999, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate',1e-6,1e-5, log=True),\n",
    "        'clip_range': trial.suggest_float('clip_range',0.1,0.4),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda',0.8,0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    try:\n",
    "\n",
    "        model_params = objective(trial)\n",
    "\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env =  DummyVecEnv([lambda:env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        print(\"made env\")\n",
    "        model = PPO('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        print(\"made model\")\n",
    "\n",
    "        model.learn(total_timesteps=100000)\n",
    "        print(\"model learned\")\n",
    "\n",
    "        mean_reward, __ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 01:47:44,605] A new study created in memory with name: no-name-b27f5974-be7d-4751-b88c-63b419cb8dfd\n",
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 7223`, after every 112 untruncated mini-batches, there will be a truncated mini-batch of size 55\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=7223 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 01:56:19,940] Trial 0 finished with value: 0.0 and parameters: {'n_steps': 7223, 'gamma': 0.8597738092524785, 'learning_rate': 1.1119985900035e-06, 'clip_range': 0.3030221233562785, 'gae_lambda': 0.8964788387717223}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 7256`, after every 113 untruncated mini-batches, there will be a truncated mini-batch of size 24\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=7256 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 02:04:38,151] Trial 1 finished with value: 2000.0 and parameters: {'n_steps': 7256, 'gamma': 0.8099543624492647, 'learning_rate': 5.330761863556447e-06, 'clip_range': 0.36752556272775394, 'gae_lambda': 0.9300076682403225}. Best is trial 1 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2161`, after every 33 untruncated mini-batches, there will be a truncated mini-batch of size 49\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2161 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 02:13:08,272] Trial 2 finished with value: 0.0 and parameters: {'n_steps': 2161, 'gamma': 0.9696700551363613, 'learning_rate': 2.3856354938769044e-06, 'clip_range': 0.2962566757048234, 'gae_lambda': 0.8878275307611758}. Best is trial 1 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6037`, after every 94 untruncated mini-batches, there will be a truncated mini-batch of size 21\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6037 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 02:21:21,853] Trial 3 finished with value: 2000.0 and parameters: {'n_steps': 6037, 'gamma': 0.9225925607306555, 'learning_rate': 4.873246617719623e-06, 'clip_range': 0.3984094530817629, 'gae_lambda': 0.872715215283243}. Best is trial 1 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2128`, after every 33 untruncated mini-batches, there will be a truncated mini-batch of size 16\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2128 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 02:28:49,737] Trial 4 finished with value: 1000.0 and parameters: {'n_steps': 2128, 'gamma': 0.9377849869946006, 'learning_rate': 3.219485200321302e-06, 'clip_range': 0.20730405370079533, 'gae_lambda': 0.8278776465180212}. Best is trial 1 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8149`, after every 127 untruncated mini-batches, there will be a truncated mini-batch of size 21\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=8149 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 02:37:07,504] Trial 5 finished with value: 1000.0 and parameters: {'n_steps': 8149, 'gamma': 0.8822822145338179, 'learning_rate': 7.271807282504287e-06, 'clip_range': 0.12416690166331842, 'gae_lambda': 0.9510671299595304}. Best is trial 1 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3630`, after every 56 untruncated mini-batches, there will be a truncated mini-batch of size 46\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3630 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 02:45:08,541] Trial 6 finished with value: 2100.0 and parameters: {'n_steps': 3630, 'gamma': 0.8234369637268758, 'learning_rate': 9.063651462172827e-06, 'clip_range': 0.3020869434870336, 'gae_lambda': 0.851760832369112}. Best is trial 6 with value: 2100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3729`, after every 58 untruncated mini-batches, there will be a truncated mini-batch of size 17\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3729 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 02:53:03,628] Trial 7 finished with value: 2000.0 and parameters: {'n_steps': 3729, 'gamma': 0.9007280153377641, 'learning_rate': 2.3914816011014576e-06, 'clip_range': 0.21240491368324968, 'gae_lambda': 0.8466024609724703}. Best is trial 6 with value: 2100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5595`, after every 87 untruncated mini-batches, there will be a truncated mini-batch of size 27\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=5595 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 03:00:44,030] Trial 8 finished with value: 3000.0 and parameters: {'n_steps': 5595, 'gamma': 0.8157202903839094, 'learning_rate': 1.154858774456118e-06, 'clip_range': 0.26012333935931625, 'gae_lambda': 0.879540718426021}. Best is trial 8 with value: 3000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/wbth78/virtualenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2521`, after every 39 untruncated mini-batches, there will be a truncated mini-batch of size 25\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2521 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 03:08:50,044] Trial 9 finished with value: 2500.0 and parameters: {'n_steps': 2521, 'gamma': 0.8639640543263564, 'learning_rate': 3.567881682657694e-06, 'clip_range': 0.15732607934744744, 'gae_lambda': 0.851311492805658}. Best is trial 8 with value: 3000.0.\n"
     ]
    }
   ],
   "source": [
    "#create the experiment/study. since returning a positive value, want to maximise the function. \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params =\"\"\"{'n_steps': 5595,\n",
    " 'gamma': 0.8157202903839094,\n",
    " 'learning_rate': 1.154858774456118e-06,\n",
    " 'clip_range': 0.26012333935931625,\n",
    " 'gae_lambda': 0.879540718426021}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the parameters from the best model\n",
    "model_params = {'n_steps': 5595,\n",
    " 'gamma': 0.8157202903839094,\n",
    " 'learning_rate': 1.154858774456118e-06,\n",
    " 'clip_range': 0.26012333935931625,\n",
    " 'gae_lambda': 0.879540718426021}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change n_steps to be a multiple of 64\n",
    "5595/64 #=87.421875\n",
    "87*64 #=5568\n",
    "model_params['n_steps'] = 5568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(os.path.join(OPT_DIR, \"trial_8_best_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls+1300000))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"./PPOtrain3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model every 100,000 steps at checkpoint_dir\n",
    "callback = TrainAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env =  DummyVecEnv([lambda:env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and train the model for 5M timesteps total\n",
    "model = PPO('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fac800bf6d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here training was accidentally cut at 1.3M timesteps so this model is loaded\n",
    "#and then training is continued for 3.7M timesteps\n",
    "model.load(os.path.join(CHECKPOINT_DIR, 'best_model_1300000.zip'))\n",
    "model.learn(total_timesteps=3700000, callback= callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fb17188cbe0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load fully trained model\n",
    "model.load('./PPOtrain3/best_model_5000000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward,_ = evaluate_policy(model, env, render=False, n_eval_episodes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13700.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print(reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
