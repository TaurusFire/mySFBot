{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import retro\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box, Discrete\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import optuna\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import tensorboard\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the viable button combinations that can be inputted on the same frame\n",
    "combos = [[\"DOWN\", \"LEFT\"],[\"DOWN\", \"LEFT\", \"A\"],[\"DOWN\", \"LEFT\", \"B\"],[\"DOWN\", \"LEFT\", \"C\"],[\"DOWN\", \"LEFT\", \"X\"],[\"DOWN\", \"LEFT\", \"Y\"], [\"DOWN\", \"LEFT\", \"Z\"], \n",
    "          [\"DOWN\", \"RIGHT\"], [\"DOWN\", \"RIGHT\", \"A\"],[\"DOWN\", \"RIGHT\", \"B\"],[\"DOWN\", \"RIGHT\", \"C\"],[\"DOWN\", \"RIGHT\", \"X\"],[\"DOWN\", \"RIGHT\", \"Y\"], [\"DOWN\", \"RIGHT\", \"Z\"], \n",
    "          [\"DOWN\"], [\"DOWN\", \"A\"], [\"DOWN\", \"B\"], [\"DOWN\", \"C\"], [\"DOWN\", \"X\"], [\"DOWN\", \"Y\"], [\"DOWN\", \"Z\"],\n",
    "          [\"UP\", \"LEFT\"],[\"UP\", \"LEFT\", \"A\"],[\"UP\", \"LEFT\", \"B\"],[\"UP\", \"LEFT\", \"C\"],[\"UP\", \"LEFT\", \"X\"],[\"UP\", \"LEFT\", \"Y\"], [\"UP\", \"LEFT\", \"Z\"], \n",
    "          [\"UP\", \"RIGHT\"], [\"UP\", \"RIGHT\", \"A\"],[\"UP\", \"RIGHT\", \"B\"],[\"UP\", \"RIGHT\", \"C\"],[\"UP\", \"RIGHT\", \"X\"],[\"UP\", \"RIGHT\", \"Y\"], [\"UP\", \"RIGHT\", \"Z\"], \n",
    "          [\"UP\"],[\"UP\", \"A\"], [\"UP\", \"B\"], [\"UP\", \"C\"], [\"UP\", \"X\"], [\"UP\", \"Y\"], [\"UP\", \"Z\"],\n",
    "          [\"LEFT\"],[\"LEFT\", \"A\"], [\"LEFT\", \"B\"], [\"LEFT\", \"C\"], [\"LEFT\", \"X\"], [\"LEFT\", \"Y\"], [\"LEFT\", \"Z\"],\n",
    "          [\"RIGHT\"],[\"RIGHT\", \"A\"], [\"RIGHT\", \"B\"], [\"RIGHT\", \"C\"], [\"RIGHT\", \"X\"], [\"RIGHT\", \"Y\"], [\"RIGHT\", \"Z\"],\n",
    "          [\"A\"],[\"B\"],[\"C\"],[\"X\"],[\"Y\"],[\"Z\"]]\n",
    "#unforunately \"do nothing\" was forgotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([0] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = 1\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class StreetFighterDiscretizer(Discretizer):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env=env, combos=combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFighter(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.observation_space = Box(low=0, high=255, \n",
    "                                     shape=(84,84,1), dtype=np.uint8)\n",
    "        \n",
    "        #12-long vector where each action corresps to a 0 or a 1\n",
    "        self.action_space = MultiBinary(12)\n",
    "        #need order of buttons so that the combos array can convert actions into the corresp multibinary array\n",
    "        self.buttons = [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
    "        \n",
    "        #start up an instance of the game\n",
    "        self.game = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\")\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        #turn to grey, resize, and regain the channels value\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), interpolation= cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels\n",
    "    \n",
    "    def step(self,action):\n",
    "        #take a step\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "\n",
    "        #preprocess the observation\n",
    "        obs = self.preprocess(obs)\n",
    "\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "\n",
    "        #reward function is score delta\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Set score to zero at start\n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logs for tensorboard data and hyperparameter models\n",
    "LOG_DIR = './DQNlogs/'\n",
    "OPT_DIR = './DQNopt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveDQN(trial):\n",
    "    #generates an example set of hyperparamaters\n",
    "    return {\n",
    "        'gamma': trial.suggest_float('gamma',0.8,0.9999, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate',1e-5,1e-4, log=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    #evaluate the agent's performance when it trains using different sets of hyperparameters\n",
    "    try:\n",
    "        model_params = objectiveDQN(trial)\n",
    "\n",
    "        env = StreetFighter()\n",
    "        env = StreetFighterDiscretizer(env)\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda:env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        \n",
    "        model = DQN('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        print(\"made model\")\n",
    "\n",
    "        model.learn(total_timesteps=100000)\n",
    "        print(\"model learned\")\n",
    "\n",
    "\n",
    "        mean_reward, __ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "    except Exception as e:\n",
    "        return -1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create the experiment/study. since returning a positive value, want to maximise the function. \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#obtain the best set of hyperparameters (trial 2) and set this to model_params\n",
    "\n",
    "study.best_params #= {'gamma': 0.8607026864367819, 'learning_rate': 1.240332072345838e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'gamma': 0.8607026864367819, 'learning_rate': 1.240332072345838e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        print(\"Checkpoint reached!\")\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls+800000))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"./DQNtrain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model every 100,000 steps at checkpoint_dir\n",
    "callback = TrainAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StreetFighter environment is passed into the discretizer to deal with the actions\n",
    "env = StreetFighter()\n",
    "env = StreetFighterDiscretizer(env)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda:env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and learn for 200,000 steps (repeated 5 times to get to 1M timesteps)\n",
    "model.load(\"./DQNtrain/best_model_800000.zip\")\n",
    "model.learn(total_timesteps=200000, callback= callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load fully trained model\n",
    "model.load(\"./DQNtrain/best_model_1000000.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain mean reward after 30 episodes\n",
    "mean_reward,_ = evaluate_policy(model, env, n_eval_episodes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Set flag to flase. Then predict using the model.\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print(reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
