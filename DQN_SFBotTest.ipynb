{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box, Discrete\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import optuna\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import tensorboard\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [[\"DOWN\", \"LEFT\"],[\"DOWN\", \"LEFT\", \"A\"],[\"DOWN\", \"LEFT\", \"B\"],[\"DOWN\", \"LEFT\", \"C\"],[\"DOWN\", \"LEFT\", \"X\"],[\"DOWN\", \"LEFT\", \"Y\"], [\"DOWN\", \"LEFT\", \"Z\"], \n",
    "          [\"DOWN\", \"RIGHT\"], [\"DOWN\", \"RIGHT\", \"A\"],[\"DOWN\", \"RIGHT\", \"B\"],[\"DOWN\", \"RIGHT\", \"C\"],[\"DOWN\", \"RIGHT\", \"X\"],[\"DOWN\", \"RIGHT\", \"Y\"], [\"DOWN\", \"RIGHT\", \"Z\"], \n",
    "          [\"DOWN\"], [\"DOWN\", \"A\"], [\"DOWN\", \"B\"], [\"DOWN\", \"C\"], [\"DOWN\", \"X\"], [\"DOWN\", \"Y\"], [\"DOWN\", \"Z\"],\n",
    "          [\"UP\", \"LEFT\"],[\"UP\", \"LEFT\", \"A\"],[\"UP\", \"LEFT\", \"B\"],[\"UP\", \"LEFT\", \"C\"],[\"UP\", \"LEFT\", \"X\"],[\"UP\", \"LEFT\", \"Y\"], [\"UP\", \"LEFT\", \"Z\"], \n",
    "          [\"UP\", \"RIGHT\"], [\"UP\", \"RIGHT\", \"A\"],[\"UP\", \"RIGHT\", \"B\"],[\"UP\", \"RIGHT\", \"C\"],[\"UP\", \"RIGHT\", \"X\"],[\"UP\", \"RIGHT\", \"Y\"], [\"UP\", \"RIGHT\", \"Z\"], \n",
    "          [\"UP\"],[\"UP\", \"A\"], [\"UP\", \"B\"], [\"UP\", \"C\"], [\"UP\", \"X\"], [\"UP\", \"Y\"], [\"UP\", \"Z\"],\n",
    "          [\"LEFT\"],[\"LEFT\", \"A\"], [\"LEFT\", \"B\"], [\"LEFT\", \"C\"], [\"LEFT\", \"X\"], [\"LEFT\", \"Y\"], [\"LEFT\", \"Z\"],\n",
    "          [\"RIGHT\"],[\"RIGHT\", \"A\"], [\"RIGHT\", \"B\"], [\"RIGHT\", \"C\"], [\"RIGHT\", \"X\"], [\"RIGHT\", \"Y\"], [\"RIGHT\", \"Z\"],\n",
    "          [\"A\"],[\"B\"],[\"C\"],[\"X\"],[\"Y\"],[\"Z\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([0] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = 1\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class StreetFighterDiscretizer(Discretizer):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env=env, combos=combos)\n",
    "class StreetFighter(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        #the observation space is a 84x84 box with each value correp to a colour\n",
    "        self.observation_space = Box(low=0, high=255, \n",
    "                                     shape=(84,84,1), dtype=np.uint8)\n",
    "        \n",
    "        #12-long vector where each action corresps to a 0 or a 1\n",
    "        self.action_space = MultiBinary(12)\n",
    "        self.buttons = [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
    "        #start up an instance of the game\n",
    "        #use restricted actions ensures that only valid button combinations are chosen\n",
    "        self.game = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\")\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        #turn to grey\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        #resize\n",
    "        resize = cv2.resize(gray, (84,84), interpolation= cv2.INTER_CUBIC)\n",
    "        #need to regain the channels value. need this for stable baselines (the RL package we use here)\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels\n",
    "        \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "\n",
    "    def step(self,action):\n",
    "        #take a step\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "\n",
    "        #want to preprocess the observation\n",
    "        obs = self.preprocess(obs)\n",
    "\n",
    "        #frame delta: pixel change\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "\n",
    "        #reshape the reward function. want the change in score, so we just subtract scores.\n",
    "        #what other info can the game give us?\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Create a attribute to hold the score delta \n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    env.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './PPOlogs/'\n",
    "OPT_DIR = './PPOopt/'\n",
    "#Create an instance of the environment\n",
    "env = StreetFighter()\n",
    "env = StreetFighterDiscretizer(env)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda:env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = DQN('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "model.load('/Users/Cheks/Desktop/Durham /Durham Part 2/Data Science/Project/mySFBot/trained_bots/DQN_1000000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "obs = env.reset()\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #time.sleep(0.005)\n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward,_ = evaluate_policy(model, env, render=False, n_eval_episodes=30)\n",
    "mean_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
