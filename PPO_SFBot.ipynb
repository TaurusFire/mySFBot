{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "import retro\n",
    "import time\n",
    "import os\n",
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create environment. call env.close() to close\n",
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        \n",
    "        #the observation space is a 84x84 box with each value correp to a colour\n",
    "        self.observation_space = Box(low=0, high=255, \n",
    "                                     shape=(84,84,1), dtype=np.uint8)\n",
    "        \n",
    "        #action space of 12-long vectors where each action corresps to a 0 or a 1\n",
    "        self.action_space = MultiBinary(12)\n",
    "\n",
    "        #start up an instance of the game\n",
    "        #use restricted actions ensures that only valid button combinations are chosen\n",
    "        self.game = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\",\n",
    "                               use_restricted_actions = retro.Actions.FILTERED)\n",
    "\n",
    "    def step(self,action):\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        #reward function is score delta\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "\n",
    "    def reset(self):\n",
    "        #Set first frame and score to zero at start\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        self.score=0\n",
    "        return obs\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        #turn to grey, resize, and regain the channels value\n",
    "\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), interpolation= cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logs for tensorboard data and hyperparameter models\n",
    "LOG_DIR = './PPOlogs/'\n",
    "OPT_DIR = './PPOopt/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    #generates an example set of hyperparamaters\n",
    "\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps',2048,8192),\n",
    "        'gamma': trial.suggest_float('gamma',0.8,0.9999, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate',1e-6,1e-5, log=True),\n",
    "        'clip_range': trial.suggest_float('clip_range',0.1,0.4),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda',0.8,0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    #evaluate the agent's performance when it trains using different sets of hyperparameters\n",
    "\n",
    "    try:\n",
    "\n",
    "        model_params = objective(trial)\n",
    "\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env =  DummyVecEnv([lambda:env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        model = PPO('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        print(\"made model\")\n",
    "\n",
    "        model.learn(total_timesteps=100000)\n",
    "        print(\"model learned\")\n",
    "\n",
    "        mean_reward, __ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create the experiment/study. since returning a positive value, want to maximise the function. \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#obtain the best set of hyperparameters (trial 8) and set this to model_params\n",
    "\n",
    "\"\"\"study.best_params = {'n_steps': 5595,\n",
    " 'gamma': 0.8157202903839094,\n",
    " 'learning_rate': 1.154858774456118e-06,\n",
    " 'clip_range': 0.26012333935931625,\n",
    " 'gae_lambda': 0.879540718426021}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'n_steps': 5595,\n",
    " 'gamma': 0.8157202903839094,\n",
    " 'learning_rate': 1.154858774456118e-06,\n",
    " 'clip_range': 0.26012333935931625,\n",
    " 'gae_lambda': 0.879540718426021}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change n_steps to be a multiple of 64\n",
    "5595/64 #=87.421875\n",
    "87*64 #=5568\n",
    "model_params['n_steps'] = 5568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where we save the intermediate saved models\n",
    "CHECKPOINT_DIR = \"./PPOtrain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model every 100,000 steps at checkpoint_dir\n",
    "callback = TrainAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env =  DummyVecEnv([lambda:env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and train the model\n",
    "model = PPO('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(os.path.join(OPT_DIR, 'trial_8_best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000000, callback= callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load fully trained model\n",
    "model.load('./PPOtrain/best_model_1000000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain mean reward after 30 episodes\n",
    "mean_reward,_ = evaluate_policy(model, env, render=False, n_eval_episodes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 800k trained model to check if reward is higher or lower\n",
    "model.load('./PPOtrain3/best_model_800000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward2,_ = evaluate_policy(model, env, render=False, n_eval_episodes=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 2.8M trained model to check if reward is higher or lower \n",
    "model.load('./PPOtrain3/best_model_2800000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward3,_ = evaluate_policy(model, env, render=False, n_eval_episodes=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print(reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
