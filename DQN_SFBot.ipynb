{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box, Discrete\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import optuna\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import tensorboard\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [[\"DOWN\", \"LEFT\"],[\"DOWN\", \"LEFT\", \"A\"],[\"DOWN\", \"LEFT\", \"B\"],[\"DOWN\", \"LEFT\", \"C\"],[\"DOWN\", \"LEFT\", \"X\"],[\"DOWN\", \"LEFT\", \"Y\"], [\"DOWN\", \"LEFT\", \"Z\"], \n",
    "          [\"DOWN\", \"RIGHT\"], [\"DOWN\", \"RIGHT\", \"A\"],[\"DOWN\", \"RIGHT\", \"B\"],[\"DOWN\", \"RIGHT\", \"C\"],[\"DOWN\", \"RIGHT\", \"X\"],[\"DOWN\", \"RIGHT\", \"Y\"], [\"DOWN\", \"RIGHT\", \"Z\"], \n",
    "          [\"DOWN\"], [\"DOWN\", \"A\"], [\"DOWN\", \"B\"], [\"DOWN\", \"C\"], [\"DOWN\", \"X\"], [\"DOWN\", \"Y\"], [\"DOWN\", \"Z\"],\n",
    "          [\"UP\", \"LEFT\"],[\"UP\", \"LEFT\", \"A\"],[\"UP\", \"LEFT\", \"B\"],[\"UP\", \"LEFT\", \"C\"],[\"UP\", \"LEFT\", \"X\"],[\"UP\", \"LEFT\", \"Y\"], [\"UP\", \"LEFT\", \"Z\"], \n",
    "          [\"UP\", \"RIGHT\"], [\"UP\", \"RIGHT\", \"A\"],[\"UP\", \"RIGHT\", \"B\"],[\"UP\", \"RIGHT\", \"C\"],[\"UP\", \"RIGHT\", \"X\"],[\"UP\", \"RIGHT\", \"Y\"], [\"UP\", \"RIGHT\", \"Z\"], \n",
    "          [\"UP\"],[\"UP\", \"A\"], [\"UP\", \"B\"], [\"UP\", \"C\"], [\"UP\", \"X\"], [\"UP\", \"Y\"], [\"UP\", \"Z\"],\n",
    "          [\"LEFT\"],[\"LEFT\", \"A\"], [\"LEFT\", \"B\"], [\"LEFT\", \"C\"], [\"LEFT\", \"X\"], [\"LEFT\", \"Y\"], [\"LEFT\", \"Z\"],\n",
    "          [\"RIGHT\"],[\"RIGHT\", \"A\"], [\"RIGHT\", \"B\"], [\"RIGHT\", \"C\"], [\"RIGHT\", \"X\"], [\"RIGHT\", \"Y\"], [\"RIGHT\", \"Z\"],\n",
    "          [\"A\"],[\"B\"],[\"C\"],[\"X\"],[\"Y\"],[\"Z\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([0] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = 1\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class StreetFighterDiscretizer(Discretizer):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env=env, combos=combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFighter(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        #the observation space is a 84x84 box with each value correp to a colour\n",
    "        self.observation_space = Box(low=0, high=255, \n",
    "                                     shape=(84,84,1), dtype=np.uint8)\n",
    "        \n",
    "        #12-long vector where each action corresps to a 0 or a 1\n",
    "        self.action_space = MultiBinary(12)\n",
    "        self.buttons = [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
    "        #start up an instance of the game\n",
    "        #use restricted actions ensures that only valid button combinations are chosen\n",
    "        self.game = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\")\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        #turn to grey\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        #resize\n",
    "        resize = cv2.resize(gray, (84,84), interpolation= cv2.INTER_CUBIC)\n",
    "        #need to regain the channels value. need this for stable baselines (the RL package we use here)\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels\n",
    "    \n",
    "    def step(self,action):\n",
    "        #take a step\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "\n",
    "        #want to preprocess the observation\n",
    "        obs = self.preprocess(obs)\n",
    "\n",
    "        #frame delta: pixel change\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "\n",
    "        #reshape the reward function. want the change in score, so we just subtract scores.\n",
    "        #what other info can the game give us?\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Create a attribute to hold the score delta \n",
    "        self.score = 0 \n",
    "        return obs\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"setting up directories so that models from any trial are \n",
    "accessible and don't have to train from the beginninbg\"\"\"\n",
    "LOG_DIR = './DQNlogs/'\n",
    "OPT_DIR = './DQNopt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveDQN(trial):\n",
    "    return {\n",
    "        'gamma': trial.suggest_float('gamma',0.8,0.9999, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate',1e-5,1e-4, log=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    #try:\n",
    "        #training loop\n",
    "        #obtain set of hyperparameters\n",
    "        model_params = objectiveDQN(trial)\n",
    "\n",
    "        #create environment\n",
    "        env = StreetFighter()\n",
    "        env = StreetFighterDiscretizer(env)\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda:env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        print(\"made env\")\n",
    "        model = DQN('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        print(\"made model\")\n",
    "\n",
    "        model.learn(total_timesteps=100000)\n",
    "        print(\"model learned\")\n",
    "\n",
    "\n",
    "        mean_reward, __ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "    #except Exception as e:\n",
    "    #    return -1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:14:13,472] A new study created in memory with name: no-name-b8b09996-348b-4f12-8ee9-def6263d1014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:19:16,162] Trial 0 finished with value: 4300.0 and parameters: {'gamma': 0.8551141150554815, 'learning_rate': 3.5999933728822436e-05}. Best is trial 0 with value: 4300.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:25:40,990] Trial 1 finished with value: 17000.0 and parameters: {'gamma': 0.9718310610932153, 'learning_rate': 6.31245955595259e-05}. Best is trial 1 with value: 17000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:30:35,731] Trial 2 finished with value: 24200.0 and parameters: {'gamma': 0.8607026864367819, 'learning_rate': 1.240332072345838e-05}. Best is trial 2 with value: 24200.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:35:40,876] Trial 3 finished with value: 20700.0 and parameters: {'gamma': 0.9192585447351365, 'learning_rate': 3.106373951152704e-05}. Best is trial 2 with value: 24200.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:40:20,675] Trial 4 finished with value: 3100.0 and parameters: {'gamma': 0.856085249635548, 'learning_rate': 2.7504609232089266e-05}. Best is trial 2 with value: 24200.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:45:08,969] Trial 5 finished with value: 4600.0 and parameters: {'gamma': 0.8776840683001154, 'learning_rate': 9.351503604307526e-05}. Best is trial 2 with value: 24200.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:49:59,701] Trial 6 finished with value: 3800.0 and parameters: {'gamma': 0.8539614240230364, 'learning_rate': 3.359381101894916e-05}. Best is trial 2 with value: 24200.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:54:52,821] Trial 7 finished with value: 600.0 and parameters: {'gamma': 0.8670854734032942, 'learning_rate': 1.319833323208281e-05}. Best is trial 2 with value: 24200.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 17:59:42,811] Trial 8 finished with value: 6000.0 and parameters: {'gamma': 0.9520029384884906, 'learning_rate': 2.3529303408603747e-05}. Best is trial 2 with value: 24200.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made env\n",
      "made model\n",
      "model learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 18:04:26,952] Trial 9 finished with value: 3200.0 and parameters: {'gamma': 0.9101693876396653, 'learning_rate': 3.8625176925141133e-05}. Best is trial 2 with value: 24200.0.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "#trying to obtain greatest mean reward\n",
    "study.optimize(optimize_agent, n_trials=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241m.\u001b[39mbest_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.8607026864367819\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.240332072345838e-05\u001b[39m}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "study.best_params #= {'gamma': 0.8607026864367819, 'learning_rate': 1.240332072345838e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'gamma': 0.8607026864367819, 'learning_rate': 1.240332072345838e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to load a model\n",
    "model = DQN.load(os.path.join(OPT_DIR, \"trial_2_best_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks are functions that is passed as an argument to another function or method\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        print(\"Checkpoint reached!\")\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls+800000))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"./DQNtrain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint reached!\n"
     ]
    }
   ],
   "source": [
    "#save the model every 10,000 steps at checkpoint_dir\n",
    "callback = TrainAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter()\n",
    "env = StreetFighterDiscretizer(env)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda:env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f1f984fa4f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN('CnnPolicy',env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "#model.load(os.path.join(OPT_DIR, 'trial_2_best_model.zip'))\n",
    "model.load(\"./DQNtrain/best_model_800000.zip\")\n",
    "model.learn(total_timesteps=200000, callback= callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(\"./DQNtrain/best_model_1000000.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15500.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Set flag to flase. Then predict using the model.\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.01)\n",
    "        print(reward)\n",
    "\n",
    "#Can also have a separate testing notebook alongside the training notebook \n",
    "# so you can test and train at the same time and check you are headed in the right \n",
    "# direction - so don't need to wait until the end to see if the model is performing well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
